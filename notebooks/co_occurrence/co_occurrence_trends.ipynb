{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02dec727",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Overview\n",
    "\n",
    "This notebook implements a processing pipeline from computes word co-occurrences from plain text. The term-term co-occurrence matrix is transformed into a DTM corpus that has a vocabulary consisting of token-pairs. The co-occurring word trends can hence be xxplored using the ordinary word trends analysis tools.\n",
    "\n",
    "For large corpora the processing time can be considerable and in such a case you\n",
    "should consider using the CLI-version of the processing pipeline.\n",
    "\n",
    "### Text Processing Pipeline\n",
    "\n",
    "| | Building block | Arguments | Description | Configuration\n",
    "| -- | :------------- | :------------- | :------------- | :------------- |\n",
    "| âš™ | <b>SetTagger</b>~~SpacyModel~~ | tagger | Set PoS tagger | spaCy\n",
    "| ðŸ“œ| <b>LoadText</b> | reader_opts, transform_opts | Load text stream | config.yml\n",
    "| ðŸ”Ž | <b>Tqdm</b> | âšª | Progress indicator | âšª\n",
    "| âŒ› | <b>Passthrough</b> | âšª | Passthrough  | âšª\n",
    "| ðŸ”¨ | Spacy<b>ToTaggedFrame</b> | tagger service | PoS tagging |\n",
    "| ðŸ’¾ | <b>Checkpoint</b> | tagged_corpus_source | Checkpoint (tagged frames) to file |\n",
    "| ðŸ”¨ | TaggedFrame<b>ToTokens</b> | extract_opts | Tokens extractor | User\n",
    "| ðŸ”¨ | <b>TokensTransform</b> | transform_opts | Tokens transformer | User\n",
    "| ðŸ”¨ | <b>Vocabulary</b> | âšª | Generate a token to integer ID mapping | âšª\n",
    "| ðŸ”¨ | <i>Partition</i> | âšª | Partition corpus into subsets based on predicate | 'year'\n",
    "| ðŸ”Ž | <i>ToTTM</i> | âšª | Transform each partition to TTM matrices | User\n",
    "| ðŸ”¨ | <b>ToCoOccurrence</b> | âšª| Transform TTM into data frame with normalized values | âšª\n",
    "| ðŸ’¾ | <i>Checkpoint</i> | checkpoint_filename| Store co-occurrence data frame | âšª\n",
    "| ðŸ”¨ | <b>ToDTM</b> | vectorize_opts| Transform data frame into DTM | âšª\n",
    "| ðŸ’¾ | <b>Checkpoint</b> | checkpoint_filename| Checkpoint (DTM) to file | âšª\n",
    "\n",
    "\n",
    "### How the co-occurrence counts are computed\n",
    "\n",
    "The co-occurrences are computed using a sliding window of size (D + 1 + D) that word by word moves through each document in the corpus, and keeps count of how many windows each pair of words co-occur in. Note that all windows will (currenty) always have an odd number of words, and the reason for this is the conditioned co-occurrence described below.\n",
    "\n",
    "The computation is done by first creating a (streamed) windowed corpus, consisting of all windows. From this corpus a DTM (document-term-matrix) is created, giving counts for each word in each window. This DTM is then used to compute a TTM (term-term-matrix) simply by multiplying the DTM with a transposed version of itself.\n",
    "\n",
    "Also note that the process of generating windows currently ignores sentences, paragraphs etc.\n",
    "\n",
    "#### GUI elements\n",
    "\n",
    "| | Config element |  Description |\n",
    "| -- | :------------- | :------------- |\n",
    "| | Corpus type | Type of corpus, disabled since only text corpora are allowed in this notebook.\n",
    "| | Source corpus file | Select file (ZIP) or folder that contains the text documents.\n",
    "| | Output tag | String that will be prefix to result files. Only valid filename chars are allowed.\n",
    "| | Output folder | Target folder for result files.\n",
    "| | Part-of-speech groups | Groups of tags to include in DTM given corpus PoS-schema\n",
    "| | Filename fields | Specifies attribute values to be extracted from filenames\n",
    "| | Lemmatize | Use word's lemma (base form)\n",
    "| | To lower | Lowercase all words\n",
    "| | Only alphabetic | Filter out words that has at least one non-alphabetic character\n",
    "| | Only alphanumeric | Filter out words that has at least one non-alphanumeric character\n",
    "| | No stopwords | Remove common stopwords using NLTK language specific stopwords\n",
    "| | Extra stopwords | Additional stopwords to remove\n",
    "| | Create subfolder | Sore result in subfolder named `output tag` in `target folder`\n",
    "| | Context distance | Max allowed distance to window's center-most word\n",
    "| | Concept | If specified, filter out windows having a center-most word not in list of specified words\n",
    "| | Remove concept | If specified, filter out concept word's co-occurrence pairs\n",
    "| | Normalize | Normalize output based on selected time period's corpus size\n",
    "| | Smooth | Smooth curve using PCHIP interpolations.\n",
    "\n",
    "\n",
    "N.B. Note that PoS schema (e.g. SUC, Universal, ON5 Penn Treebank tag sets) and language must be set for each corpus. This, and other options, is specified in the _corpus configuration file_. For an example, please see _SSI.yml_ inf the `resources` folder.\n",
    "\n",
    "\n",
    "### Concept co-occurrence\n",
    "\n",
    "Concept co-occurrence is a __conditioned__ co-occurrence where the set of windows are constrained so that the center-most word must one of a number of specified (concept) words. This results is a set of co-occurrences that occur in close proximity of the center-most word (i.e. having max distance of D to center-most word).\n",
    "\n",
    "### Co-occurrence trends\n",
    "\n",
    "Displays word co-occurrence trends using a set different keyness metrics. Specifiy words of interest in the `Words to find` text box. You can use both __wildcards__ ```*``` and __regular expressions__ to widen your search. The \"words\" in this case are co-occurrence pairs represented as a token \"word1/word2\". To find instances matching \"information\" you could enter ```information*```, ```*information``` or ```*information*``` to match pairs starting with information, ending with information or containing information respectively. The words in the vocabulary that matches what you have specified will be listed under `Matched words` when you press __TAB__. Since using wildcards and regexps can result\n",
    "in a large number of words, only the `Word count` most frequent words are displayed. Refine your search if you get to many matches. The words are plotted when you selected them in the `Matched words` list. Select and plot __multiple words__ by pressing CTRL, or using CTRL + arrow keys. The word-pairs are listed in descending order by their global corpus frequency.\n",
    "\n",
    "\n",
    "#### GUI elements\n",
    "\n",
    "| | Config element |  Description |\n",
    "| -- | :------------- | :------------- |\n",
    "| | Keyness | Specifies how metric/keyness to use when weighing the co-occurrence values.\n",
    "| | Keyness source | Which corpus to use when computing keyness.\n",
    "| | Top count | Max number of matching co-occurring word pairs to display\n",
    "| | Grouping | Target folder for result files.\n",
    "| | Normalize | Normalize entire corpus for selected period (after keyness has been computed)\n",
    "| | Auto | Automatic update (compute) whenever a value is changed (currently disabled)\n",
    "| | Smooth | Smooth trend values (lines) using interpolation.\n",
    "| | Compute | Compute new data based on current selections.\n",
    "| | Words to find | Specifies word-pairs(s) to find.\n",
    "| | Matched words | Words (i.e. word-pairs) in vocabulary that matches `Words to find`.\n",
    "\n",
    "The regular expressions must be surrounded by vertical bars ```|```. To find words ending with ```tion```\n",
    "you can enter ```|.*tion$|```  The vertical\n",
    "bars is specified only so that the system can distinguish the regexp from normal words. The actual expression is ```^.*tion$```.\n",
    "The dot and star ```.*``` matches any character (the dot) any number of times (the ```*```). The dollar sign ```$``` indicates the word ending.\n",
    "So this expression matches all words that begins with any number of characters follwoed, by the character sequence ```tion``` at the end of the word.\n",
    "To match all words starting with `info`you can enter ```|^info.*|``` where ```^``` specifies the start of the word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import __paths__\n",
    "from bokeh.plotting import output_notebook\n",
    "from IPython.display import display\n",
    "from penelope.notebook.co_occurrence import MainGUI\n",
    "\n",
    "__paths__.data_folder = \"/data/inidun\"\n",
    "__paths__.resources_folder = \"/data/inidun/resources\"\n",
    "\n",
    "output_notebook()\n",
    "gui = MainGUI(\n",
    "    corpus_config=\"courier_article_pages\",\n",
    "    corpus_folder=__paths__.corpus_folder,\n",
    "    data_folder=__paths__.data_folder,\n",
    "    resources_folder=__paths__.resources_folder,\n",
    ")\n",
    "display(gui.layout())"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
