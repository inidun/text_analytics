{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.2 64-bit",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "feaa6644a67d98bc2c380c25a7ae5b1ab301b95bfc0add23663419f9cdbe38f8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Setup notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-09-30 16:30:38,069 : INFO : Loading model: en...\n2020-09-30 16:30:38,070 : INFO : Using pipeline: tagger parser\n2020-09-30 16:30:38,071 : INFO : Call time [setup_nlp_language_model]: 0.0023 secs\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "assert sys.version_info > (3,8)\n",
    "\n",
    "root_folder = os.path.join(os.getcwd().split('text_analytics')[0], 'text_analytics')\n",
    "sys.path = sys.path + [ root_folder, globals()['_dh'][-1] ]\n",
    "\n",
    "import text_analytic_tools.utility.utils as utility\n",
    "import text_analytic_tools.common.textacy_utility as textacy_utility\n",
    "\n",
    "corpus_folder = os.path.join(root_folder, 'data')\n",
    "source_path = os.path.join(corpus_folder, 'legal_instrument_corpus.zip')\n",
    "prepped_source_path = utility.path_add_suffix(source_path, '_preprocessed')\n",
    "\n",
    "language = 'en'\n",
    "nlp = textacy_utility.setup_nlp_language_model(language, disable=('ner', ))\n",
    "textacy_corpus_path = textacy_utility.generate_corpus_filename(prepped_source_path, language)\n",
    "document_index = pd.read_csv(os.path.join(corpus_folder, 'legal_instrument_index.csv'), sep=';', header=0)"
   ]
  },
  {
   "source": [
    "## Prepare and load `SSI Legal Intruments` corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-09-30 16:30:50,699 : INFO : Preparing text corpus...\n"
    }
   ],
   "source": [
    "import text_analytic_tools.common.text_corpus as text_corpus\n",
    "\n",
    "def get_document_stream(prepped_source_path, document_index):\n",
    "\n",
    "    reader = text_corpus.CompressedFileReader(prepped_source_path)\n",
    "    document_index = document_index.set_index('filename')\n",
    "\n",
    "    for document_name, text in reader:\n",
    "\n",
    "        metadata = document_index.loc[document_name].to_dict()\n",
    "        document_id = metadata['unesco_id']\n",
    "\n",
    "        yield document_name, document_id, text, metadata\n",
    "\n",
    "if not os.path.isfile(prepped_source_path):\n",
    "    textacy_utility.preprocess_text(source_path, prepped_source_path)\n",
    "\n",
    "if not os.path.isfile(textacy_corpus_path):\n",
    "\n",
    "    stream = get_document_stream(prepped_source_path, document_index)\n",
    "    textacy_corpus = textacy_utility.create_textacy_corpus(stream, nlp)\n",
    "   \n",
    "    textacy_corpus.save(textacy_corpus_path)\n",
    "\n",
    "else:\n",
    "    textacy_corpus = textacy_utility.load_corpus(textacy_corpus_path, nlp)\n",
    "\n",
    "print(\"Done! \")\n"
   ]
  },
  {
   "source": [
    "## Stats"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "POS_TO_COUNT = {\n",
    "    'SYM': 0, 'PART': 0, 'ADV': 0, 'NOUN': 0, 'CCONJ': 0, 'ADJ': 0, 'DET': 0, 'ADP': 0, 'INTJ': 0, 'VERB': 0, 'NUM': 0, 'PRON': 0, 'PROPN': 0\n",
    "}\n",
    "\n",
    "POS_NAMES = list(sorted(POS_TO_COUNT.keys()))\n",
    "\n",
    "def get_pos_statistics(doc):   \n",
    "    \n",
    "    pos_iter = ( x.pos_ for x in doc if x.pos_ not in ['NUM', 'PUNCT', 'SPACE'] )       \n",
    "    pos_counts = dict(collections.Counter(pos_iter))\n",
    "    stats = utility.extend(\n",
    "        dict(\n",
    "            document_id=doc.user_data['textacy']['meta']['document_id']),\n",
    "            dict(POS_TO_COUNT),\n",
    "            pos_counts\n",
    "        )    \n",
    "    return stats\n",
    "\n",
    "def get_corpus_documents(corpus, document_index):\n",
    "    \n",
    "    metadata = [ get_pos_statistics(doc) for doc in corpus ]\n",
    "    df = pd.DataFrame(metadata).set_index('document_id')\n",
    "    df = df.merge(document_index, how='inner', left_index=True, right_on='unesco_id')\n",
    "    df['words'] = df[POS_NAMES].apply(sum, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_corpus_statistics(    \n",
    "    documents_index,\n",
    "    corpus,\n",
    "    group_by_column='year',\n",
    "    target='lemma',\n",
    "    include_pos=None,\n",
    "    stop_words=None\n",
    "):  \n",
    "\n",
    "    documents = get_corpus_documents(textacy_corpus, document_index)\n",
    "    value_columns = list(textacy_utility.POS_NAMES) if (len(include_pos or [])) == 0 else list(include_pos)\n",
    "    \n",
    "    documents['signed_lustrum'] = (documents.year - documents.year.mod(5)).astype(int)\n",
    "    documents['signed_decade'] = (documents.year - documents.year.mod(10)).astype(int)\n",
    "    documents['total'] = documents[value_columns].apply(sum, axis=1)\n",
    "    \n",
    "    aggregates = { x: ['sum'] for x in value_columns }\n",
    "    aggregates['total'] = ['sum', 'mean', 'min', 'max', 'size' ]\n",
    "    \n",
    "    documents = documents.groupby(group_by_column).agg(aggregates)\n",
    "    documents.columns = [ ('Total, ' + x[1].lower()) if x[0] == 'total' else x[0] for x in documents.columns ]\n",
    "    columns = sorted(value_columns) + sorted([ x for x in documents.columns if x.startswith('Total')])\n",
    "    return documents[columns]\n",
    "\n",
    "corpus_stats = compute_corpus_statistics(document_index, textacy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebooks.word_trends.ipyaggrid_plot as ipyaggrid_plot\n",
    "display(ipyaggrid_plot.simple_plot(corpus_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}